<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>http://andife.github.io/onnx.github.io/videos/</title><link href="https://andife.github.io/onnx.github.io/docs/" rel="alternate"></link><link href="https://andife.github.io/onnx.github.io/docs/feeds/speaker_haihao-shen-intel-china.atom.xml" rel="self"></link><id>https://andife.github.io/onnx.github.io/docs/</id><updated>2021-03-10T00:00:00+00:00</updated><entry><title>Quantization support for ONNX using Intel Neural Compressor (formerly named Intel Low Precision Optimization Tool)</title><link href="https://andife.github.io/onnx.github.io/docs/onnx-community-day-2021_03/quantization-support-for-onnx-using-intel-neural-compressor-formerly-named-intel-low-precision-optimization-tool.html" rel="alternate"></link><updated>2021-03-10T00:00:00+00:00</updated><author><name>Haihao Shen (Intel - China)</name></author><id>tag:andife.github.io,2021-03-10:onnx.github.io/docs/onnx-community-day-2021_03/quantization-support-for-onnx-using-intel-neural-compressor-formerly-named-intel-low-precision-optimization-tool.html</id><summary type="html"></summary><category term="intel"></category><category term="quantization"></category></entry></feed>