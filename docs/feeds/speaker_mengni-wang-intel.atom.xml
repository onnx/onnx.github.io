<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>http://andife.github.io/onnx.github.io/videos/</title><link href="https://andife.github.io/onnx.github.io/docs/" rel="alternate"></link><link href="https://andife.github.io/onnx.github.io/docs/feeds/speaker_mengni-wang-intel.atom.xml" rel="self"></link><id>https://andife.github.io/onnx.github.io/docs/</id><updated>2021-10-24T00:00:00+00:00</updated><entry><title>Intel Neural Compressor: A Scalable Quantization Tool for ONNX Models</title><link href="https://andife.github.io/onnx.github.io/docs/onnx-community-day-2021_10/intel-neural-compressor-a-scalable-quantization-tool-for-onnx-models.html" rel="alternate"></link><updated>2021-10-24T00:00:00+00:00</updated><author><name>Mengni Wang (Intel)</name></author><id>tag:andife.github.io,2021-10-24:onnx.github.io/docs/onnx-community-day-2021_10/intel-neural-compressor-a-scalable-quantization-tool-for-onnx-models.html</id><summary type="html"></summary><category term="Intel"></category><category term="INC"></category><category term="Quantization"></category></entry></feed>