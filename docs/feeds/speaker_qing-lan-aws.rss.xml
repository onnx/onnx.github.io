<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>http://andife.github.io/onnx.github.io/videos/</title><link>https://andife.github.io/onnx.github.io/docs/</link><description></description><atom:link href="https://andife.github.io/onnx.github.io/docs/feeds/speaker_qing-lan-aws.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 24 Jun 2022 00:00:00 +0000</lastBuildDate><item><title>Build your high-performance model inference solution with DJL and ONNX Runtime</title><link>https://andife.github.io/onnx.github.io/docs/onnx-community-day-2022_06/build-your-high-performance-model-inference-solution-with-djl-and-onnx-runtime.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In many companies, Java is the primary language for the teams to build up services. To have ONNX model onboard and integration, developers faced several technical challenges on the resource allocation and performance tuning. In this talk, we will walk you through the inference solution built by DJL, a ML library in Java. In the meantime, we will share some customer success stories with model hosting using ONNXRuntime and DJL.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Qing Lan (AWS)</dc:creator><pubDate>Fri, 24 Jun 2022 00:00:00 +0000</pubDate><guid>tag:andife.github.io,2022-06-24:onnx.github.io/docs/onnx-community-day-2022_06/build-your-high-performance-model-inference-solution-with-djl-and-onnx-runtime.html</guid><category>High-performance</category><category>djl</category></item></channel></rss>