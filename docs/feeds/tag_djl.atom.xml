<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>http://andife.github.io/onnx.github.io/videos/</title><link href="https://andife.github.io/onnx.github.io/" rel="alternate"></link><link href="https://andife.github.io/onnx.github.io/feeds/tag_djl.atom.xml" rel="self"></link><id>https://andife.github.io/onnx.github.io/</id><updated>2022-06-24T00:00:00+00:00</updated><entry><title>Build your high-performance model inference solution with DJL and ONNX Runtime</title><link href="https://andife.github.io/onnx.github.io/onnx-community-day-2022_06/build-your-high-performance-model-inference-solution-with-djl-and-onnx-runtime.html" rel="alternate"></link><updated>2022-06-24T00:00:00+00:00</updated><author><name>Qing Lan (AWS)</name></author><id>tag:andife.github.io,2022-06-24:onnx.github.io/onnx-community-day-2022_06/build-your-high-performance-model-inference-solution-with-djl-and-onnx-runtime.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In many companies, Java is the primary language for the teams to build up services. To have ONNX model onboard and integration, developers faced several technical challenges on the resource allocation and performance tuning. In this talk, we will walk you through the inference solution built by DJL, a ML library in Java. In the meantime, we will share some customer success stories with model hosting using ONNXRuntime and DJL.&lt;/p&gt;
</summary><category term="High-performance"></category><category term="djl"></category></entry></feed>