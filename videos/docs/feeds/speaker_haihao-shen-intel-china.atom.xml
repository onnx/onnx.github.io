<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>https://onnx.ai/media/</title><link href="/" rel="alternate"></link><link href="/feeds/speaker_haihao-shen-intel-china.atom.xml" rel="self"></link><id>/</id><updated>2021-03-10T00:00:00+00:00</updated><entry><title>Quantization support for ONNX using Intel Neural Compressor (formerly named Intel Low Precision Optimization Tool)</title><link href="/onnx-community-day-2021_03/quantization-support-for-onnx-using-intel-neural-compressor-formerly-named-intel-low-precision-optimization-tool.html" rel="alternate"></link><updated>2021-03-10T00:00:00+00:00</updated><author><name>Haihao Shen (Intel - China)</name></author><id>tag:,2021-03-10:onnx-community-day-2021_03/quantization-support-for-onnx-using-intel-neural-compressor-formerly-named-intel-low-precision-optimization-tool.html</id><summary type="html"></summary><category term="intel"></category><category term="quantization"></category></entry></feed>