<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>https://onnx.ai/media/</title><link href="/" rel="alternate"></link><link href="/feeds/speaker_viet-yen-nguyen-hypefactors.atom.xml" rel="self"></link><id>/</id><updated>2022-06-24T00:00:00+00:00</updated><entry><title>Billions of NLP Inferences on the JVM using ONNX and DJL</title><link href="/onnx-community-day-2022_06/billions-of-nlp-inferences-on-the-jvm-using-onnx-and-djl.html" rel="alternate"></link><updated>2022-06-24T00:00:00+00:00</updated><author><name>Viet Yen Nguyen (Hypefactors)</name></author><id>tag:,2022-06-24:onnx-community-day-2022_06/billions-of-nlp-inferences-on-the-jvm-using-onnx-and-djl.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This session outlines the recently rolled out Hypefactors' MLOps infrastructure designed for billions NLP inferences a day. The workload serves media intelligence and OSINT use cases. The infrastructure is designed with a Java Virtual Machine-first approach that is enabled by ONNX interop and AWS' Deep Java Library. On top of that, we show how quantization drives further performance optimizations.&lt;/p&gt;
</summary><category term="nlp"></category></entry></feed>