<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>https://onnx.ai/media/</title><link href="/" rel="alternate"></link><link href="/feeds/tag_edge.atom.xml" rel="self"></link><id>/</id><updated>2022-06-24T00:00:00+00:00</updated><entry><title>Deploy ONNX model seamlessly across the cloud, edge, and mobile devices using MindSpore</title><link href="/onnx-community-day-2021_03/deploy-onnx-model-seamlessly-across-the-cloud-edge-and-mobile-devices-using-mindspore.html" rel="alternate"></link><updated>2021-03-10T00:00:00+00:00</updated><author><name>Leon Wang (Huawei-China)</name></author><id>tag:,2021-03-10:onnx-community-day-2021_03/deploy-onnx-model-seamlessly-across-the-cloud-edge-and-mobile-devices-using-mindspore.html</id><summary type="html"></summary><category term="cloud"></category><category term="edge"></category></entry><entry><title>Using ONNX with Qualcomm powered devices from smartphones to the cloud edge and everything in between.</title><link href="/onnx-community-day-2022_06/using-onnx-with-qualcomm-powered-devices-from-smartphones-to-the-cloud-edge-and-everything-in-between.html" rel="alternate"></link><updated>2022-06-24T00:00:00+00:00</updated><author><name>Felix Baum (Qualcomm)</name></author><id>tag:,2022-06-24:onnx-community-day-2022_06/using-onnx-with-qualcomm-powered-devices-from-smartphones-to-the-cloud-edge-and-everything-in-between.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Whenever our clients target high performant AI cloud inferencing servers, create new and exciting AI based experiences on mobile phones or improve our lives by adding more and more AI features into cars, many of them use ONNX models as an interchange format. Qualcomm helps to deploy and accelerate natural language processing, computer vision, classification, segmentation, and transformer based models in various verticals: Mobile, IoT, XR, Compute and Automotive. We created a link between ONNX and Qualcomm AI Engine direct that allows us to run the same model not only on various backends such as CPU, GPU, Hexagon processor or Low Power AI subsystem of the same SoC, and migrate it to run on range of the devices due to the portability that ONNX provides. In addition to the above, we would briefly cover in this session the work we are doing with Microsoft on collaboration for ONNX RT Execution Provider for a range of our AI accelerators.&lt;/p&gt;
</summary><category term="edge"></category></entry></feed>