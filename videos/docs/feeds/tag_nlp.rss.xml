<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>https://onnx.ai/media/</title><link>/</link><description></description><atom:link href="/feeds/tag_nlp.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 24 Jun 2022 00:00:00 +0000</lastBuildDate><item><title>Billions of NLP Inferences on the JVM using ONNX and DJL</title><link>/onnx-community-day-2022_06/billions-of-nlp-inferences-on-the-jvm-using-onnx-and-djl.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This session outlines the recently rolled out Hypefactors' MLOps infrastructure designed for billions NLP inferences a day. The workload serves media intelligence and OSINT use cases. The infrastructure is designed with a Java Virtual Machine-first approach that is enabled by ONNX interop and AWS' Deep Java Library. On top of that, we show how quantization drives further performance optimizations.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Viet Yen Nguyen (Hypefactors)</dc:creator><pubDate>Fri, 24 Jun 2022 00:00:00 +0000</pubDate><guid>tag:,2022-06-24:onnx-community-day-2022_06/billions-of-nlp-inferences-on-the-jvm-using-onnx-and-djl.html</guid><category>nlp</category></item></channel></rss>