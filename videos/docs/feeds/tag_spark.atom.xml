<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>https://onnx.ai/media/</title><link href="/" rel="alternate"></link><link href="/feeds/tag_spark.atom.xml" rel="self"></link><id>/</id><updated>2022-06-24T00:00:00+00:00</updated><entry><title>Bring the power of ONNX to Spark as it never happened before</title><link href="/onnx-community-day-2022_06/bring-the-power-of-onnx-to-spark-as-it-never-happened-before.html" rel="alternate"></link><updated>2022-06-24T00:00:00+00:00</updated><author><name>Xiyuan Wang (Huawei)</name></author><id>tag:,2022-06-24:onnx-community-day-2022_06/bring-the-power-of-onnx-to-spark-as-it-never-happened-before.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Both data processing platforms and deep learning frameworks are evolving in their own fields. Usually, Spark is used for offline data processing, and then various deep learning frameworks are used for data inference. A simplified API for DL Inferencing is very important as a bridge. What does an ideal data and deep learning inference pipeline look like? We'll discuss how to build your AI application using Spark and ONNX, the current status and initial idea of Spark community to improve this pipeline, and also make full use of the capabilities of Ascend Hardware Platform. This topic help you know the latest progress of Ascend Hardware Platform integration in ONNX, as well as the initial idea of the inference pipeline improvement in the Spark community.&lt;/p&gt;
</summary><category term="spark"></category></entry></feed>