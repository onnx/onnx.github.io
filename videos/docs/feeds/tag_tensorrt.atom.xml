<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>https://onnx.ai/media/</title><link href="/" rel="alternate"></link><link href="/feeds/tag_tensorrt.atom.xml" rel="self"></link><id>/</id><updated>2022-06-24T00:00:00+00:00</updated><entry><title>INT8 Inference of Quantization-Aware trained models using ONNX-TensorRT</title><link href="/onnx-community-day-2022_06/int8-inference-of-quantization-aware-trained-models-using-onnx-tensorrt.html" rel="alternate"></link><updated>2022-06-24T00:00:00+00:00</updated><author><name>Dheeraj Peri (NVIDIA)</name></author><id>tag:,2022-06-24:onnx-community-day-2022_06/int8-inference-of-quantization-aware-trained-models-using-onnx-tensorrt.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Accelerating Deep Neural Networks (DNN) inference is an important step in realizing latency-critical deployment of real-world applications such as image classification, image segmentation, natural language processing, etc. The need to improve DNN's inference latency has sparked interest in running those models in lower precisions, such as FP16 and INT8. In particular, running DNNs in INT8 precision can offer faster inference and a much lower memory footprint than its floating-point counterpart. NVIDIA TensorRT supports Quantization-Aware Training (QAT) techniques to convert floating-point DNN models to INT8 precision. In this talk, we shall demonstrate end-end workflow of converting Tensorflow QAT models into ONNX, which is a standard intermediate representation to deploy using TensorRT. We use TF2ONNX package to convert a quantized Tensorflow model into ONNX. ONNX format makes it easier to visualize graphs via netron which can provide users information about placement of quantized nodes.&lt;/p&gt;
</summary><category term="int8"></category><category term="qat"></category><category term="tensorrt"></category></entry></feed>